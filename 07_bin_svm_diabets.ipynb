{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOziFVWKAXjbbVpWQEfwu8r"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector Machine (SVM) Binary Classification\n",
        "\n",
        "This notebook demonstrates the implementation of **Support Vector Machine (SVM)** for binary classification using the **Pima Indian Diabetes Prediction Dataset**. SVM is a robust supervised learning algorithm that excels at classification tasks, particularly when dealing with high-dimensional and non-linearly separable data.\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. **What is SVM?**\n",
        "\n",
        "**Support Vector Machine (SVM)** is a powerful machine learning algorithm designed to classify data by finding the optimal decision boundary that separates classes. The key idea of SVM is to maximize the **margin**, which is the distance between the decision boundary (hyperplane) and the nearest data points from each class, called **support vectors**.\n",
        "\n",
        "Key Features of SVM:\n",
        "- **Maximizing Margin**: Ensures better generalization by finding the hyperplane that best separates the classes.\n",
        "- **Kernel Trick**: Maps data into higher dimensions to enable linear separation of non-linear data. Common kernels include:\n",
        "  - `Linear`: Works when the data is linearly separable.\n",
        "  - `RBF (Radial Basis Function)`: Useful for non-linear data and is widely used.\n",
        "  - `Polynomial`: Fits non-linear data with polynomial relationships.\n",
        "- **Regularization Parameter (C)**: Controls the trade-off between maximizing the margin and minimizing classification errors.\n",
        "- **Gamma**: Defines how far the influence of a single data point reaches, particularly in RBF and polynomial kernels.\n",
        "\n",
        "SVM is especially effective in scenarios with smaller datasets and a clear margin of separation between classes.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Steps Implemented in this Notebook**\n",
        "\n",
        "1. **Dataset Loading and Exploration**:\n",
        "   - The **Pima Indian Diabetes Prediction Dataset** is loaded to predict diabetes based on features like glucose level, BMI, and age.\n",
        "   - The dataset consists of two classes: `0` (No Diabetes) and `1` (Diabetes).\n",
        "\n",
        "2. **Data Preprocessing**:\n",
        "   - **Normalization**: Feature scaling is performed using standardization to improve SVMâ€™s performance, as it is sensitive to the range of feature values.\n",
        "   - Handling missing or invalid values, such as replacing zero BMI with the median or mean.\n",
        "\n",
        "3. **Train-Test Split**:\n",
        "   - The dataset is split into training and testing sets in an 80:20 ratio, ensuring a fair evaluation of the model's performance.\n",
        "\n",
        "4. **Model Training**:\n",
        "   - An SVM model is trained using the **RBF kernel** to handle non-linear decision boundaries.\n",
        "   - Hyperparameters (`C` and `gamma`) are tuned to optimize performance.\n",
        "\n",
        "5. **Model Evaluation**:\n",
        "   - Performance metrics such as **accuracy, precision, recall, and F1-score** are used to evaluate the model.\n",
        "   - A confusion matrix is generated to visualize the classification results, including true positives, false positives, true negatives, and false negatives.\n",
        "\n",
        "6. **Insights and Visualization**:\n",
        "   - Analyzes the influence of hyperparameters and kernels on the classification task.\n",
        "   - Visualizes the decision boundaries (for reduced feature dimensions) to understand how SVM separates the two classes.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Why Use SVM?**\n",
        "\n",
        "- SVM is ideal for binary classification tasks where the margin of separation is critical.\n",
        "- Its flexibility to handle non-linear data using kernels makes it a versatile choice.\n",
        "- SVM is less prone to overfitting, especially when properly tuned, and works well even with a smaller number of samples.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wWnAslkPbJmp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rig3tLiTaSce"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Collection"
      ],
      "metadata": {
        "id": "2Kg58IYPcC2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/devdio/flyai_datasets/main/diabetes.csv"
      ],
      "metadata": {
        "id": "KnMp1PrDcCJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'diabetes.csv'\n",
        "diabetes = pd.read_csv(path)\n",
        "diabetes.shape"
      ],
      "metadata": {
        "id": "K6wKCSSacbgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes.head()"
      ],
      "metadata": {
        "id": "j2vIv5oRc7s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = diabetes.copy()\n",
        "df.info() # -> There's no missing data, all of them are numeric variables(No need to encode)"
      ],
      "metadata": {
        "id": "jCFIVoGmc-Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "id": "GRVvFi1ld7Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorical variables"
      ],
      "metadata": {
        "id": "q_awJnwkdkn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set(df.columns)"
      ],
      "metadata": {
        "id": "66wOjGQdemyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Outcome'].value_counts()"
      ],
      "metadata": {
        "id": "QImI6x9_epYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=df, x='Outcome')"
      ],
      "metadata": {
        "id": "pNPycbW1etME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Continuous variables"
      ],
      "metadata": {
        "id": "Q8pKAmLzfBo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = df['Pregnancies'].sort_values(ascending=False)\n",
        "tmp = tmp.reset_index()\n",
        "tmp.head()"
      ],
      "metadata": {
        "id": "DQdZv48se-lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x=tmp.index, y = tmp['Pregnancies'])"
      ],
      "metadata": {
        "id": "SUYzLv6Uff94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist()"
      ],
      "metadata": {
        "id": "Z5n3eepBgv9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing datas"
      ],
      "metadata": {
        "id": "ZPHc4QZRhBwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "NJu5Ry1RhBEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Duplication"
      ],
      "metadata": {
        "id": "_xFh4HcRhJop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "m2ErIj06hIc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outlier"
      ],
      "metadata": {
        "id": "oHtpI7JFhREQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw boxplot\n",
        "df.boxplot(figsize=(10,10))"
      ],
      "metadata": {
        "id": "2jPV8H_ChP-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "id": "bL2ZO0oxhU2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Separate train, test data."
      ],
      "metadata": {
        "id": "2YjTFY64hrQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.1, random_state=SEED, stratify=df['Outcome'])\n",
        "train.shape, test.shape"
      ],
      "metadata": {
        "id": "mpm82IhRhmRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['Outcome'].value_counts()"
      ],
      "metadata": {
        "id": "KfrdUc2ziauG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "mh2EviKujaGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separate variables x and y."
      ],
      "metadata": {
        "id": "342Ue_zEjmvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train.drop('Outcome', axis=1)\n",
        "y_train = train['Outcome']\n",
        "\n",
        "X_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "rISeZ5JajoT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Replace outliers with 0 values to a specific value (median)."
      ],
      "metadata": {
        "id": "cTLd5OV11K09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'"
      ],
      "metadata": {
        "id": "fJyfqvZNmUSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "median_list = []\n",
        "\n",
        "col_list = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "for col in col_list:\n",
        "  med = X_train[col].median()\n",
        "  X_train.loc[X_train[col] == 0, col] = med\n",
        "  median_list.append(med)"
      ],
      "metadata": {
        "id": "u_7nVQmxnHnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm that there are no values where the minimum is 0.\n",
        "X_train.describe().T"
      ],
      "metadata": {
        "id": "zcV2nt5goOO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling"
      ],
      "metadata": {
        "id": "KpwusbWgohr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "ss = StandardScaler()\n",
        "X_train_s = ss.fit_transform(X_train)\n",
        "X_train_s # it's automatically changed to np.array"
      ],
      "metadata": {
        "id": "KfGp-aSaoUjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ss.mean_) # Each col's mean value\n",
        "print(ss.var_) # Each columns's variance"
      ],
      "metadata": {
        "id": "74hRPBwvpSRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_e = y_train.to_numpy()\n",
        "y_train_e"
      ],
      "metadata": {
        "id": "hdaAZ54-pxlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_s.shape, y_train_e.shape)\n",
        "print(type(X_train_s), type(y_train_e))"
      ],
      "metadata": {
        "id": "dB82haKxp3Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Learning"
      ],
      "metadata": {
        "id": "ZmL3Z8T-qKdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "clf = SVC(random_state=SEED)\n",
        "clf.fit(X_train_s, y_train_e)"
      ],
      "metadata": {
        "id": "Tfx4X7v5rLZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Validation"
      ],
      "metadata": {
        "id": "MoGbVmebqFvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test.drop('Outcome', axis=1)\n",
        "y_test = test['Outcome']\n",
        "\n",
        "X_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "r7IwADVkrryP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data proprecessing(test)\n",
        "# median_list = []\n",
        "col_list = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "for i, col in enumerate(col_list):\n",
        "  X_test.loc[X_test[col] == 0, col] = median_list[i]\n",
        "  median_list.append(med)"
      ],
      "metadata": {
        "id": "ljq1rOcfrOpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_s = ss.transform(X_test)\n",
        "X_test_s"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cuKVsT-1sDGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_e = y_test.to_numpy()\n",
        "y_test_e"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XKHhXrI7sTsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test_s.shape, y_test_e.shape)\n",
        "print(type(X_test_s), type(y_test_e))"
      ],
      "metadata": {
        "id": "rt-1bb2VsWMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test_s)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "E0Z2hBwEsqcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define a function to print classification metrics and display a confusion matrix heatmap\n",
        "def print_metrics(y_true, y_pred, ave='binary'):\n",
        "    print('accuracy:', accuracy_score(y_test_e, y_pred))\n",
        "    print('recall:', recall_score(y_test_e, y_pred, average=ave))\n",
        "    print('precision:', precision_score(y_test_e, y_pred, average=ave))\n",
        "    print('f1 :', f1_score(y_test_e, y_pred, average=ave))\n",
        "\n",
        "    # Generate and display the confusion matrix as a heatmap\n",
        "    clm = confusion_matrix(y_test_e, y_pred)\n",
        "    s = sns.heatmap(clm, annot=True, fmt='d', cbar=False)\n",
        "    s.set(xlabel='Predicted', ylabel='Actual')  # Set axis labels\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "idIrF9OOs1mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_metrics(y_test_e, y_pred)"
      ],
      "metadata": {
        "id": "s3p-Z_8LtiOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.Model tuning"
      ],
      "metadata": {
        "id": "yE7xPuwat-9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define a parameter grid for hyperparameter tuning\n",
        "prams_grid = {\n",
        "    'C': [0.01, 0.02, 0.05, 0.1, 0.5, 1, 10, 100],  # Regularization parameter\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],  # Kernel coefficient\n",
        "    'kernel': ['rbf', 'poly']  # Types of kernel functions\n",
        "}\n",
        "\n",
        "# Initialize the SVC model with a fixed random seed for reproducibility\n",
        "clf = SVC(random_state=SEED)\n",
        "\n",
        "# Set up GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=clf,               # Model to be optimized\n",
        "    param_grid=prams_grid,       # Parameter grid to search\n",
        "    cv=3,                        # 3-fold cross-validation\n",
        "    n_jobs=-1,                   # Use all available CPU cores for parallel processing\n",
        "    refit=True,                  # Refit the model with the best parameters on the entire training data\n",
        "    verbose=2,                   # Increase verbosity for progress updates\n",
        "    return_train_score=True      # Include training scores in the results\n",
        ")\n",
        "\n",
        "# Perform grid search and fit the model on the training data\n",
        "grid_search.fit(X_train_s, y_train_e)\n"
      ],
      "metadata": {
        "id": "RtxrGMtrt9we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best estimator (model) from the grid search\n",
        "# This will provide the SVC model with the optimal hyperparameters found during grid search\n",
        "grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "3ga5l3QGwY_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best hyperparameters found during the grid search\n",
        "# This will return a dictionary containing the optimal parameter values for the model\n",
        "grid_search.best_params_"
      ],
      "metadata": {
        "id": "U8CT0YhcwgD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the best estimator (model with optimal hyperparameters) to make predictions on the test data\n",
        "y_pred = grid_search.best_estimator_.predict(X_test_s)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "T34Oom_awlBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model's performance using the custom print_metrics function\n",
        "# This will display accuracy, recall, precision, F1 score, and the confusion matrix heatmap\n",
        "print_metrics(y_test_e, y_pred)"
      ],
      "metadata": {
        "id": "Zj0u9HcnwsLv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}